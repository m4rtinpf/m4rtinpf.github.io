{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "loan_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hx39MVHFPrT"
      },
      "source": [
        "# Introduction\n",
        "## Purpose\n",
        "The purpose of this article is to make a portfolio project to showcase some basic machine learning skills.\n",
        "\n",
        "To do this I used a [tutorial](https://www.dataquest.io/blog/data-science-portfolio-machine-learning/) from Dataquest.\n",
        "\n",
        "This article will be a rehash of the Dataquest tutorial project. Then, in another, separate, article, I will make a different project with the same format.\n",
        "\n",
        "The source code of this project is also available on [GitHub](https://github.com/m4rtinpf/loan-prediction).\n",
        "\n",
        "\n",
        "## Topic\n",
        "According to [Investopedia](https://www.investopedia.com/articles/investing/091814/fannie-mae-what-it-does-and-how-it-operates.asp):\n",
        "\n",
        ">Fannie Mae is a government-sponsored enterprise that makes mortgages available to low- and moderate-income borrowers. It does not provide loans, but backs or guarantees them in the secondary mortgage market.\n",
        "\n",
        "The goal of this project is to predict if a loan acquired by Fannie Mae will go into foreclosure or not.\n",
        "\n",
        "## Basic datasets\n",
        "Fannie Mae publishes [here](https://capitalmarkets.fanniemae.com/credit-risk-transfer/single-family-credit-risk-transfer/fannie-mae-single-family-loan-performance-data) data for the loans that it has acquired and how they perform through time.\n",
        "\n",
        "In 2020, Fannie Mae modified the way they publish their data. To avoid having to make too many modifications to the tutorial project, I used the [old dataset](https://rapidsai.github.io/demos/datasets/mortgage-data).\n",
        "\n",
        "## Project structure\n",
        "In this case, the project is going to be structured using different Python script `.py` files, instead of a Jupyter Notebook. We are going to do this so the project can be easily run in an automated way, instead of interactively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-_VViKBIABA"
      },
      "source": [
        "# Getting the data\n",
        "## Organising the files\n",
        "First, we'll create a directory called `loan-prediction`. And then another directory inside of it, called `data`.\n",
        "\n",
        "```\n",
        "mkdir loan-prediction\n",
        "cd loan-prediction\n",
        "mkdir data\n",
        "cd data\n",
        "```\n",
        "\n",
        "## Downloading the data\n",
        "The following script downloads a `gzip` compressed `tar` file containing the data, from 2000 to 2015. It does so like this:\n",
        "\n",
        "* Checks if the file already exists, using the `os.path.isfile()` function.\n",
        "  \n",
        "  If it doesn't exist, it gets the file from the `url` using `urllib.request.urlopen()`, and saves it using `shutil.copyfileobj()`.\n",
        "\n",
        "  ```\n",
        "url = 'http://rapidsai-data.s3-website.us-east-2.amazonaws.com/notebook-mortgage-data/mortgage_2000-2015.tgz'\n",
        "file_name = url.split('/')[-1]\n",
        "# Check if the file exists\n",
        "if not os.path.isfile(file_name):\n",
        "    # Download the file from \"url\" and save it as \"file_name\"\n",
        "    with urllib.request.urlopen(url) as response, open(file_name, 'wb') as out_file:\n",
        "        shutil.copyfileobj(response, out_file)\n",
        "```\n",
        "\n",
        "* Uses the `tar` library to extract only the files to be used (we will only use the same as the tutorial did).\n",
        "\n",
        "  ```\n",
        "extract_path = 'data'\n",
        "# Open the tar file\n",
        "tar = tarfile.open(file_name, \"r\")\n",
        "# Loop through each file and extract only the ones that are needed\n",
        "for f in [\n",
        "    'acq/Acquisition_2015Q1.txt',\n",
        "    'acq/Acquisition_2014Q4.txt',\n",
        "    'acq/Acquisition_2014Q3.txt',\n",
        "    'acq/Acquisition_2014Q2.txt',\n",
        "    'acq/Acquisition_2014Q1.txt',\n",
        "    'acq/Acquisition_2013Q4.txt',\n",
        "    'acq/Acquisition_2013Q3.txt',\n",
        "    'acq/Acquisition_2013Q2.txt',\n",
        "    'acq/Acquisition_2013Q1.txt',\n",
        "    'acq/Acquisition_2012Q4.txt',\n",
        "    'acq/Acquisition_2012Q3.txt',\n",
        "    'acq/Acquisition_2012Q2.txt',\n",
        "    'acq/Acquisition_2012Q1.txt',\n",
        "    'perf/Performance_2015Q1.txt',\n",
        "    'perf/Performance_2014Q4.txt',\n",
        "    'perf/Performance_2014Q3.txt',\n",
        "    'perf/Performance_2014Q2.txt',\n",
        "    'perf/Performance_2014Q1.txt',\n",
        "    'perf/Performance_2013Q4.txt',\n",
        "    'perf/Performance_2013Q3.txt_1',\n",
        "    'perf/Performance_2013Q3.txt_0',\n",
        "    'perf/Performance_2013Q2.txt_1',\n",
        "    'perf/Performance_2013Q2.txt_0',\n",
        "    'perf/Performance_2013Q1.txt_1',\n",
        "    'perf/Performance_2013Q1.txt_0',\n",
        "    'perf/Performance_2012Q4.txt_1',\n",
        "    'perf/Performance_2012Q4.txt_0',\n",
        "    'perf/Performance_2012Q3.txt_1',\n",
        "    'perf/Performance_2012Q3.txt_0',\n",
        "    'perf/Performance_2012Q2.txt_1',\n",
        "    'perf/Performance_2012Q2.txt_0',\n",
        "    'perf/Performance_2012Q1.txt_1',\n",
        "    'perf/Performance_2012Q1.txt_0',\n",
        "]:\n",
        "    tar.extract(f, path=extract_path)\n",
        "```\n",
        "\n",
        "* Removes the `tar` file as it's no longer needed.\n",
        "\n",
        "  ```\n",
        "os.remove(file_name)\n",
        "```\n",
        "\n",
        "* Moves the extracted text files to the `data` directory (essentially, it removes the directory structure that the compressed file had).\n",
        "\n",
        "  ```\n",
        "# Move files from \"acq\" and \"perf\" directories to \"data\"\n",
        "for directory in ['acq', 'perf']:\n",
        "    for f in os.listdir('{0}/{1}'.format(extract_path, directory)):\n",
        "        shutil.move('{0}/{1}/{2}'.format(extract_path, directory, f), extract_path)\n",
        "```\n",
        "\n",
        "* Removes the no longer used directories inside of `data`.\n",
        "\n",
        "  ```\n",
        "os.rmdir('{0}/{1}'.format(extract_path, directory))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QNIGGIBKpLY"
      },
      "source": [
        "# Installing the requirements\n",
        "We now go back to the `loan-prediction` directory with `cd ..`.\n",
        "\n",
        "## Requirements file\n",
        "To make it easier for user to install of required libraries, we are create a text file called `requirements.txt`. This text file contains the name of a needed library in each line:\n",
        "\n",
        "```\n",
        "pandas\n",
        "matplotlib\n",
        "scikit-learn\n",
        "numpy\n",
        "ipython\n",
        "scipy\n",
        "```\n",
        "\n",
        "## Installing\n",
        "Now, installing the requirements is as easy as running `pip install -r requirements.txt`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNvFCSygMObn"
      },
      "source": [
        "# Configuration file\n",
        "To keep all the settings of the process in one place, we'll make a `settings.py` file to specify:\n",
        "\n",
        "* The directory for the data files.\n",
        "\n",
        "  ```\n",
        "DATA_DIR = \"data\"\n",
        "```\n",
        "\n",
        "* The directory for the processed files.\n",
        "\n",
        "  ```\n",
        "PROCESSED_DIR = \"processed\"\n",
        "```\n",
        "\n",
        "* The minimum amount of quarters that a loan has to be in the dataset in order to use it.\n",
        "\n",
        "  ```\n",
        "MINIMUM_TRACKING_QUARTERS = 4\n",
        "```\n",
        "\n",
        "* The name of the target variable.\n",
        "\n",
        "  ```\n",
        "TARGET = \"foreclosure_status\"\n",
        "```\n",
        "\n",
        "* The name of the variables which are not predictors.\n",
        "\n",
        "  ```\n",
        "NON_PREDICTORS = [TARGET, \"id\"]\n",
        "```\n",
        "\n",
        "* The number of cross-validation folds to use.\n",
        "\n",
        "  ```\n",
        "CV_FOLDS = 3\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErZtLTanLuOK"
      },
      "source": [
        "# Assembling the datasets\n",
        "Right now there are a lot of `Acquisition` and `Performance` files in the `data` directory, one for each quarter.\n",
        "\n",
        "We want to combine these into two files: `Acquisition.txt` and `Performance.txt`.\n",
        "\n",
        "The text files don't have headers, so we'll define a `HEADERS` dictionary, with the name of the two types of datasets as keys, and the names of the columns of each one as values. The headers can be found [here](https://s3.amazonaws.com/dq-blog-files/lppub_file_layout.pdf).\n",
        "\n",
        "```\n",
        "HEADERS = {\n",
        "    \"Acquisition\": [\n",
        "        \"id\",\n",
        "        \"channel\",\n",
        "        \"seller\",\n",
        "        \"interest_rate\",\n",
        "        \"balance\",\n",
        "        \"loan_term\",\n",
        "        \"origination_date\",\n",
        "        \"first_payment_date\",\n",
        "        \"ltv\",\n",
        "        \"cltv\",\n",
        "        \"borrower_count\",\n",
        "        \"dti\",\n",
        "        \"borrower_credit_score\",\n",
        "        \"first_time_homebuyer\",\n",
        "        \"loan_purpose\",\n",
        "        \"property_type\",\n",
        "        \"unit_count\",\n",
        "        \"occupancy_status\",\n",
        "        \"property_state\",\n",
        "        \"zip\",\n",
        "        \"insurance_percentage\",\n",
        "        \"product_type\",\n",
        "        \"co_borrower_credit_score\"\n",
        "    ],\n",
        "    \"Performance\": [\n",
        "        \"id\",\n",
        "        \"reporting_period\",\n",
        "        \"servicer_name\",\n",
        "        \"interest_rate\",\n",
        "        \"balance\",\n",
        "        \"loan_age\",\n",
        "        \"months_to_maturity\",\n",
        "        \"maturity_date\",\n",
        "        \"msa\",\n",
        "        \"delinquency_status\",\n",
        "        \"modification_flag\",\n",
        "        \"zero_balance_code\",\n",
        "        \"zero_balance_date\",\n",
        "        \"last_paid_installment_date\",\n",
        "        \"foreclosure_date\",\n",
        "        \"disposition_date\",\n",
        "        \"foreclosure_costs\",\n",
        "        \"property_repair_costs\",\n",
        "        \"recovery_costs\",\n",
        "        \"misc_costs\",\n",
        "        \"tax_costs\",\n",
        "        \"sale_proceeds\",\n",
        "        \"credit_enhancement_proceeds\",\n",
        "        \"repurchase_proceeds\",\n",
        "        \"other_foreclosure_proceeds\",\n",
        "        \"non_interest_bearing_balance\",\n",
        "        \"principal_forgiveness_balance\"\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "To improve the performance of the algorithm, we specify the data-type of each column in each dataset.\n",
        "\n",
        "```\n",
        "TYPES = {\n",
        "    \"Acquisition\": {\n",
        "        'id': 'int64',\n",
        "        'channel': 'object',\n",
        "        'seller': 'object',\n",
        "        'interest_rate': 'float64',\n",
        "        'balance': 'int64',\n",
        "        'loan_term': 'int64',\n",
        "        'origination_date': 'object',\n",
        "        'first_payment_date': 'object',\n",
        "        'ltv': 'int64',\n",
        "        'cltv': 'float64',\n",
        "        'borrower_count': 'float64',\n",
        "        'dti': 'float64',\n",
        "        'borrower_credit_score': 'float64',\n",
        "        'first_time_homebuyer': 'object',\n",
        "        'loan_purpose': 'object',\n",
        "        'property_type': 'object',\n",
        "        'unit_count': 'int64',\n",
        "        'occupancy_status': 'object',\n",
        "        'property_state': 'object',\n",
        "        'zip': 'int64',\n",
        "        'insurance_percentage': 'float64',\n",
        "        'product_type': 'object',\n",
        "        'co_borrower_credit_score': 'float64',\n",
        "    },\n",
        "    \"Performance\": {\n",
        "        'id': 'int64',\n",
        "        'reporting_period': 'object',\n",
        "        'servicer_name': 'object',\n",
        "        'interest_rate': 'float64',\n",
        "        'balance': 'float64',\n",
        "        'loan_age': 'float64',\n",
        "        'months_to_maturity': 'float64',\n",
        "        'maturity_date': 'float64',\n",
        "        'msa': 'object',\n",
        "        'delinquency_status': 'float64',\n",
        "        'modification_flag': 'int64',\n",
        "        'zero_balance_code': 'object',\n",
        "        'zero_balance_date': 'float64',\n",
        "        'last_paid_installment_date': 'object',\n",
        "        'foreclosure_date': 'object',\n",
        "        'disposition_date': 'object',\n",
        "        'foreclosure_costs': 'object',\n",
        "        'property_repair_costs': 'float64',\n",
        "        'recovery_costs': 'float64',\n",
        "        'misc_costs': 'float64',\n",
        "        'tax_costs': 'float64',\n",
        "        'sale_proceeds': 'float64',\n",
        "        'credit_enhancement_proceeds': 'float64',\n",
        "        'repurchase_proceeds': 'float64',\n",
        "        'other_foreclosure_proceeds': 'float64',\n",
        "        'non_interest_bearing_balance': 'float64',\n",
        "        'principal_forgiveness_balance': 'float64',\n",
        "    },\n",
        "}\n",
        "```\n",
        "\n",
        "Now we make a dictionary of the columns we wish to keep: all the columns from the `Acquisition` datasets, to have as much data to predict as possible; and only the `id` and `foreclosure_date` from the `Performance` datasets.\n",
        "\n",
        "```\n",
        "SELECT = {\n",
        "    \"Acquisition\": HEADERS[\"Acquisition\"],\n",
        "    \"Performance\": [\n",
        "        \"id\",\n",
        "        \"foreclosure_date\"\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "To actually join the files, we are going to make a function called `concatenate()`, that takes `prefix` as an argument.\n",
        "\n",
        "The function will do this:\n",
        "* Get a list of the files in `DATA_DIR`.\n",
        "\n",
        "  ```\n",
        "files = os.listdir(settings.DATA_DIR)\n",
        "```\n",
        "\n",
        "*  Loop through the files:\n",
        "  * If the file name starts with `prefix`, read it as a `DataFrame`.\n",
        "  * Keep only the columns on `SELECT`.\n",
        "  * Write the `DataFrame` to a `.csv` file; only use headers for the first write.\n",
        "\n",
        "  ```\n",
        "def concatenate(prefix=\"Acquisition\"):\n",
        "    files = os.listdir(settings.DATA_DIR)\n",
        "    for f in files:\n",
        "        if not f.startswith(prefix):\n",
        "            continue\n",
        "        data = pd.read_csv(os.path.join(settings.DATA_DIR, f), sep=\"|\", header=None, names=HEADERS[prefix],\n",
        "                           index_col=False, dtype=TYPES[prefix])\n",
        "        data = data[SELECT[prefix]]\n",
        "        if not os.path.isfile(os.path.join(settings.PROCESSED_DIR, \"{}.txt\".format(prefix))):\n",
        "            data.to_csv(os.path.join(settings.PROCESSED_DIR, \"{}.txt\".format(prefix)), sep=\"|\", header=SELECT[prefix],\n",
        "                        index=False)\n",
        "        else:\n",
        "            data.to_csv(os.path.join(settings.PROCESSED_DIR, \"{}.txt\".format(prefix)), mode='a', sep=\"|\", header=False,\n",
        "                        index=False)\n",
        "```\n",
        "\n",
        "We only want this script to call the function if it is run, not if it is imported. So we'll call the function inside this conditional:\n",
        "\n",
        "```\n",
        "if __name__ == \"__main__\":\n",
        "    concatenate(\"Acquisition\")\n",
        "    concatenate(\"Performance\")\n",
        "```\n",
        "\n",
        "To make it easy to run this step, we can put all the code on a single file called `assemble.py`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D90XSTJzRnx9"
      },
      "source": [
        "# Generating the training data\n",
        "Now we have two files on the `processed` directory: `Acquisition.txt` and `Performance.txt`.\n",
        "\n",
        "But the machine learning models that we want to use need a single variable as input data.\n",
        "\n",
        "We are going to create an `annotate.py` file, with five functions in it:\n",
        "\n",
        "* `read()`\n",
        "* `count_performance_rows()`\n",
        "* `get_performance_summary_value()`\n",
        "* `annotate()`\n",
        "* `write()`\n",
        "\n",
        "## `read()`\n",
        "This function simply reads in the `Acquisition` data.\n",
        "\n",
        "```\n",
        "def read():\n",
        "    acquisition = pd.read_csv(os.path.join(settings.PROCESSED_DIR, \"Acquisition.txt\"), sep=\"|\")\n",
        "    return acquisition\n",
        "```\n",
        "\n",
        "## `count_performance_rows()`\n",
        "Its job is to count how many times the loan appears on the `Performance` dataset and check if it was foreclosed on.\n",
        "\n",
        "To do this it:\n",
        "\n",
        "* Opens the `Performance.txt` file.\n",
        "\n",
        "  ```\n",
        "with open(os.path.join(settings.PROCESSED_DIR, \"Performance.txt\"), 'r') as f:\n",
        "```\n",
        "\n",
        "* Loops through each line in the file, skipping the first (header row).\n",
        "\n",
        "  ```\n",
        "    for i, line in enumerate(f):\n",
        "            if i == 0:\n",
        "                continue\n",
        "```\n",
        "\n",
        "* Checks if the loan is in the `counts` dictionary and adds it if it isn't.\n",
        "\n",
        "  ```\n",
        "            loan_id, date = line.split(\"|\")\n",
        "            loan_id = int(loan_id)\n",
        "            if loan_id not in counts:\n",
        "                counts[loan_id] = {\n",
        "                    \"foreclosure_status\": False,\n",
        "                    \"performance_count\": 0\n",
        "                }\n",
        "```\n",
        "* Increases the `performance_count` value for the loan of the current line on the `counts` dictionary by one.\n",
        "\n",
        "  ```\n",
        "            counts[loan_id][\"performance_count\"] += 1\n",
        "```       \n",
        "\n",
        "* If the foreclosure date is not zero, sets the `foreclosure_status` of the loan to `True`.\n",
        "\n",
        "  ```\n",
        "            if len(date.strip()) > 0:\n",
        "                counts[loan_id][\"foreclosure_status\"] = True\n",
        "```\n",
        "\n",
        "* Returns the `counts` dictionary.\n",
        "\n",
        "  ```\n",
        "    return counts  \n",
        "```\n",
        "      \n",
        "## `get_performance_summary_value()`\n",
        "Returns the key corresponding to the loan provided if it exists on the `performance_summary` dictionary, or a default value if it doesn't.\n",
        "\n",
        "```\n",
        "def get_performance_summary_value(loan_id, key, performance_summary):\n",
        "    value = performance_summary.get(loan_id, {\n",
        "        \"foreclosure_status\": False,\n",
        "        \"performance_count\": 0\n",
        "    })\n",
        "    return value[key]\n",
        "```    \n",
        "\n",
        "## `annotate()`\n",
        "Creates two new columns:\n",
        "* `foreclosure_status`: contains a `Boolean` value that tells us if the loan was foreclosed on or not.\n",
        "\n",
        "  ```\n",
        "acquisition[\"foreclosure_status\"] = acquisition[\"id\"].apply(\n",
        "        lambda x: get_performance_summary_value(x, \"foreclosure_status\", performance_summary))\n",
        "```\n",
        "\n",
        "* `performance_count`: contains the number of times that a loan appears on the dataset.\n",
        "\n",
        "  ```\n",
        "acquisition[\"performance_count\"] = acquisition[\"id\"].apply(\n",
        "        lambda x: get_performance_summary_value(x, \"performance_count\", performance_summary))\n",
        "```\n",
        "\n",
        "It also loops through each numerical column and converts them to categorical.\n",
        "\n",
        "```\n",
        "    for column in [\n",
        "        \"channel\",\n",
        "        \"seller\",\n",
        "        \"first_time_homebuyer\",\n",
        "        \"loan_purpose\",\n",
        "        \"property_type\",\n",
        "        \"occupancy_status\",\n",
        "        \"property_state\",\n",
        "        \"product_type\"\n",
        "    ]:\n",
        "        acquisition[column] = acquisition[column].astype('category').cat.codes\n",
        "```\n",
        "\n",
        "And splits the date columns into year and month columns, deleting the original date columns.\n",
        "\n",
        "```\n",
        "    for start in [\"first_payment\", \"origination\"]:\n",
        "        column = \"{}_date\".format(start)\n",
        "        acquisition[\"{}_year\".format(start)] = pd.to_numeric(acquisition[column].str.split('/').str.get(1))\n",
        "        acquisition[\"{}_month\".format(start)] = pd.to_numeric(acquisition[column].str.split('/').str.get(0))\n",
        "        del acquisition[column]\n",
        "```        \n",
        "\n",
        "Now it fills all `NA` values with `-1`.\n",
        "\n",
        "```\n",
        "    acquisition = acquisition.fillna(-1)\n",
        "```\n",
        "\n",
        "Finally, it keeps only the loans that were kept for a minimum amount of quarters.\n",
        "\n",
        "```\n",
        "    acquisition = acquisition[acquisition[\"performance_count\"] > settings.MINIMUM_TRACKING_QUARTERS]\n",
        "```    \n",
        "\n",
        "## `write()`\n",
        "Writes the resulting dataset.\n",
        "\n",
        "```\n",
        "def write(acquisition):\n",
        "    acquisition.to_csv(os.path.join(settings.PROCESSED_DIR, \"train.csv\"), index=False)\n",
        "```\n",
        "\n",
        "## Calling the functions\n",
        "Again, we don't want the functions to be called if the file is imported.\n",
        "\n",
        "```\n",
        "if __name__ == \"__main__\":\n",
        "    acquisition = read()\n",
        "    performance_summary = count_performance_rows()\n",
        "    acquisition = annotate(acquisition, performance_summary)\n",
        "    write(acquisition)\n",
        "```    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxocRtIY_KuU"
      },
      "source": [
        "# Creating a machine learning model\n",
        "After running `annotate.py` we get a `train.csv` file on the `processed` directory. Now we can use this file to train our machine learning model and make predictions.\n",
        "\n",
        "We are going to use the most simple model for this type of problem: [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression).\n",
        "\n",
        "To better organise the script (located on `predict.py`), we are going to, again, make several functions:\n",
        "* `read()`\n",
        "* `cross_validate()`\n",
        "* `compute_error()`\n",
        "* `compute_false_negatives()`\n",
        "* `compute_false_positives()`\n",
        "\n",
        "## Reading in the data\n",
        "The `read()` function loads the `train.csv` file.\n",
        "\n",
        "```\n",
        "def read():\n",
        "    train = pd.read_csv(os.path.join(settings.PROCESSED_DIR, \"train.csv\"))\n",
        "    return train\n",
        "```\n",
        "\n",
        "## Performing cross-validation\n",
        "The `cross_validate()` function takes the `train` `DataFrame` as argument and:\n",
        "\n",
        "* Instantiates a `LogisticRegression` model from [`scikit-learn`](https://scikit-learn.org/).\n",
        "\n",
        "  ```\n",
        "  clf = LogisticRegression(random_state=1, class_weight=\"balanced\", solver='saga', n_jobs=-1, dual=False, tol=0.01)\n",
        "  ```\n",
        "\n",
        "  * The `random_state=1` parameter sets the seed for the pseudo-random number generator, so the result is repeatable.\n",
        "  * `class_weight=\"balanced\"` is used so that the algorithm takes into account the class imbalance (there are much fewer foreclosed on loans than not foreclosed on).\n",
        "  * `solver='saga'` tells sklearn to use the [SAGA solver](https://arxiv.org/pdf/1407.0202.pdf), which performs better for large datasets.\n",
        "  * With `n_jobs=-1` the computations will use all the microprocessor's cores.\n",
        "  * `tol=0.01` sets the stopping tolerance.\n",
        "* Gets the names of the columns to use as predictors.\n",
        "\n",
        "  ```\n",
        "  predictors = train.columns.tolist()\n",
        "  predictors = [p for p in predictors if p not in settings.NON_PREDICTORS]\n",
        "```    \n",
        "* Performs n-fold cross-validation, where n is set in the `settings.py` file.\n",
        "    \n",
        "  ```\n",
        "    predictions = model_selection.cross_val_predict(estimator=clf, X=train[predictors], y=train[settings.TARGET],\n",
        "    cv=settings.CV_FOLDS, n_jobs=-1)\n",
        "```                                \n",
        "\n",
        "## Computing error metrics\n",
        "### Accuracy score\n",
        "First, we compute the subset accuracy, defined as the proportion of correctly predicted foreclosed loans.\n",
        "\n",
        "The `compute_error()` function computes the [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html).\n",
        "\n",
        "```\n",
        "def compute_error(target, predictions):\n",
        "    return metrics.accuracy_score(target, predictions)\n",
        "```    \n",
        "\n",
        "Previously we noted that there is much class imbalance. For example, let's say that `99%` of values on the `foreclosure_status` column equal to `False`. Then we could make a model that predicts always `foreclosure_status` equals to `False` and it would be `99%` accurate. It would also be completely useless!\n",
        "\n",
        "### False negatives\n",
        "A more useful metric is the false negatives rate `FNR`. A false negative is a result that was predicted as negative (`foreclosure_status=False`), but it was actually positive (`foreclosure_status=True`).\n",
        "\n",
        "$$\n",
        "    FNR=\\frac{FN}{FN+TP}=\\frac{FN}{P}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "* `FN`: false negatives\n",
        "* `TP`: true positives\n",
        "* `P`: positives\n",
        "\n",
        "The `compute_false_negatives()` function calculates the false negatives rate.\n",
        "\n",
        "```\n",
        "def compute_false_negatives(target, predictions):\n",
        "    df = pd.DataFrame({\"target\": target, \"predictions\": predictions})\n",
        "    return df[(df[\"target\"] == 1) & (df[\"predictions\"] == 0)].shape[0] / (df[(df[\"target\"] == 1)].shape[0] + 1)\n",
        "```    \n",
        "\n",
        "### False positives\n",
        "Similarly, we can compute the false positives rate `FPR`. A false positive is a result that was predicted as positive (`foreclosure_status=True`), but it was actually negative (`foreclosure_status=False`).\n",
        "\n",
        "$$\n",
        "    FPR=\\frac{FP}{TN+FP}=\\frac{FP}{N}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "* `FP`: false positives\n",
        "* `TN`: true negatives\n",
        "* `N`: negatives\n",
        "\n",
        "The `compute_false_positives()` function calculates the false negatives rate.\n",
        "\n",
        "```\n",
        "def compute_false_positives(target, predictions):\n",
        "    df = pd.DataFrame({\"target\": target, \"predictions\": predictions})\n",
        "    return df[(df[\"target\"] == 0) & (df[\"predictions\"] == 1)].shape[0] / (df[(df[\"target\"] == 0)].shape[0] + 1)\n",
        "```\n",
        "\n",
        "## Calling the functions\n",
        "As in the previous scripts, we need to actually call the functions.\n",
        "\n",
        "```\n",
        "if __name__ == \"__main__\":\n",
        "    train = read()\n",
        "    predictions = cross_validate(train)\n",
        "    error = compute_error(train[settings.TARGET], predictions)\n",
        "    fn = compute_false_negatives(train[settings.TARGET], predictions)\n",
        "    fp = compute_false_positives(train[settings.TARGET], predictions)\n",
        "    print(\"Accuracy Score: {}\".format(error))\n",
        "    print(\"False Negatives: {}\".format(fn))\n",
        "    print(\"False Positives: {}\".format(fp))\n",
        "```    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k6T8aBCOPGk"
      },
      "source": [
        "# Predicting with the model\n",
        "To make predictions with our model, we need to run the script with `python predict.py`. We get this output after doing it:\n",
        "\n",
        "```\n",
        "Accuracy Score: 0.6590917718026998\n",
        "False Negatives: 0.2350684017350684\n",
        "False Positives: 0.34100154945883804\n",
        "```\n",
        "\n",
        "With (most of) the default parameters, we got a `66%` accuracy, `24%` false negatives rate, and `34%` false positives rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBcw_EhUPaUT"
      },
      "source": [
        "# Conclusion\n",
        "In this notebook, we created a machine learning model, going through the whole process workflow:\n",
        "* Getting the raw data.\n",
        "* Selecting the useful columns and discarding the rest.\n",
        "* Making a training dataset.\n",
        "* Training the model.\n",
        "* Making predictions.\n",
        "* Computing error metrics.\n",
        "\n",
        "There is much room for improvement with this model, so we could perform [hyperparameter tuning](https://en.wikipedia.org/wiki/Hyperparameter_optimization).\n",
        "\n",
        "This was a first approach to machine learning, done following a tutorial, and it should serve as a basis to perform a more custom-made project."
      ]
    }
  ]
}